{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "\n",
    "### Due: Tues Nov. 23rd @ 11:59pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework we will be performing \n",
    "\n",
    "- feature cleaning and engineering\n",
    "\n",
    "- dimensionality reduction with feature selection and extraction\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Follow the comments below and fill in the blanks (\\_\\_\\_\\_) to complete.\n",
    "- Where not specified, please run functions with default argument settings.\n",
    "- Please **'Restart and Run All'** prior to submission.\n",
    "- **Save pdf in Landscape** and **check that all of your code is shown** in the submission.\n",
    "- When submitting in Gradescope, be sure to **select which page corresponds to which question.**\n",
    "\n",
    "Out of 50 points total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. (1pts) Set up our environment with comman libraries and plotting.\n",
    "#    Note: generally we would do all of our imports here but some imports\n",
    "#    have been left till later where they are used.\n",
    "\n",
    "# Import numpy, pandas, matplotlib.pyplot and seaborn with our usual aliases.\n",
    "____\n",
    "\n",
    "# Execute the matplotlib magic function to display plots inline\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Cleaning and Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will be loading, cleaning and transforming a small set of data related to loan applications.\n",
    "\n",
    "There are two files, one containing loan application information and the other containing borrower information.\n",
    "\n",
    "You will need to load both files, join them and then transform this data, creating a new dataframe with features which could then be used for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. (1pts) Load Loan Application Data\n",
    "\n",
    "# Read in the first dataframe containing loan application information.\n",
    "# The path to the datafile is '../data/hw3_loan.csv'.\n",
    "# Use the appropriate pandas command to read a csv file with default arguments.\n",
    "# Store this dataframe as df_loan.\n",
    "____\n",
    "\n",
    "# Print the .info() of df_loan and check the size \n",
    "#   (should be 663 rows, 4 columns, 2 columns with missing values)\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. (2pts) Check for Duplicates and Set Index\n",
    "\n",
    "# Assert that there are no duplicates in the CustomerID column of df_loan\n",
    "____\n",
    "\n",
    "# Set the index of df_loan to the CustomerID column to make joining easier\n",
    "#    use .set_index()\n",
    "#    drop the original index\n",
    "#    store as df_loan (either overwrite variable or use inplace=True)\n",
    "____\n",
    "\n",
    "# Display the first 3 rows of df_loan to visually confirm that the index has been set\n",
    "# Note that CustomerID starts at 2 instead of 0\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. (2pts) Load Borrower Data\n",
    "\n",
    "# Read in a second table containing borrower information.\n",
    "# The path to the datafile is '../data/hw3_borrower.csv'.\n",
    "# Use the appropriate pandas command to read a csv file.\n",
    "# IMPORTANT: set 'CustomerID' index using the index_col= argument.\n",
    "# Store this dataframe as df_borrower.\n",
    "____\n",
    "\n",
    "# Print the .info() of df_borrower (should be 633 rows, 1 column w/ no missing values)\n",
    "# Note that the index has also been set (should be 663 entries, 2 to 750)\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. (2pts) Join Datasets\n",
    "\n",
    "# Join the df_loan and df_borrower\n",
    "# Perform a left join, with df_loan as the \"left\" table \n",
    "#    and df_borrower as the right.\n",
    "# Since the dataframes share an index (CustomerID), it is convenient \n",
    "#    to use the .join() function instead of .merge().\n",
    "# Store the resulting dataframe as df\n",
    "____\n",
    "\n",
    "# Print the .info() of df\n",
    "# There should still be 663 rows with 4 columns, 2 with missing values\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. (1pts) Create df_features\n",
    "\n",
    "# We are performing the transformations below in order to use this data for modeling.\n",
    "#\n",
    "# Instead of adding transformed features into our original dataframe (df)\n",
    "#   it is convenient to create a new dataframe containing only features.\n",
    "# This will eventually be the X features for our models.\n",
    "\n",
    "# Create a new, empty, DataFrame called df_features\n",
    "#   that has the same index as df (index=df.index)\n",
    "____\n",
    "\n",
    "# Print the .info() of df_features\n",
    "# The index should match the index of df above, but otherwise be empty\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration and Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RequestedAmount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. (2pts) Fill Missing Values in RequestedAmount\n",
    "\n",
    "# RequestedAmount is a numeric feature with missing values\n",
    "# Before filling the missing values we should create a dummy variable\n",
    "#   to capture which rows had missing values\n",
    "\n",
    "# We want to store this as an int instead of a boolean.\n",
    "# Use .isna().astype(int) on the RequestedAmount column \n",
    "#   to both find null values and convert bool to int.\n",
    "# Store in df_features as 'RequestedAmount_missing'.\n",
    "____\n",
    "\n",
    "# Print the number of 0s and 1s in the RequestedAmount_missing column using .value_counts().\n",
    "#   (There should be 12 1s indicated 12 missing values)\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. (1pts) Plot RequestedAmount\n",
    "\n",
    "# Use seaborn histplot to plot df.RequestedAmount using default settings.\n",
    "# Note that this feature is right skewed and has a wide range.\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. (2pts) Fill Missing Values in RequestedAmount\n",
    "\n",
    "# As RequestedAmount is right skewed, we'll fill missing values using median.\n",
    "# Use fillna() to fill the missing values in RequestedAmount \n",
    "#   with the median of RequestedAmount\n",
    "# We'll make one more transformation to this column before storing it as a feature\n",
    "#   so store back into df as df['RequestedAmount'] or use inplace=True\n",
    "____\n",
    "\n",
    "# Use assert and the sum of .isna() to check that there\n",
    "#    are no longer any missing values in RequestedAmount\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10. (2pts) Log Transform RequestedAmount\n",
    "\n",
    "# Using .apply(), apply np.log to the RequestedAmount column.\n",
    "# Store the result back into df as RequestedAmount_log\n",
    "____\n",
    "\n",
    "# Use seaborn histplot to plot RequestedAmount_log using default settings.\n",
    "# Note that the shape is is closer to a normal distribution\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. (3pts) Center and Scale RequestedAmount_log Using StandardScaler\n",
    "\n",
    "# Import StandardScaler from sklearn\n",
    "____\n",
    "\n",
    "# Using StandardScaler (with default settings) \n",
    "#   run fit_transform to standardize RequestedAmount_log\n",
    "# Note that fit_transform expects a DataFrame not a Series.\n",
    "# Use df[['RequestedAmount_log']] to get a dataframe instead of a Series.\n",
    "# Store the result in df_features as 'ReqAmount_logscaled'\n",
    "____\n",
    "\n",
    "# Confirm that scaling has been applied properly by printing out \n",
    "#    the 'mean' and 'std' of df_features.RequestedAmount_logscaled\n",
    "#    using the .agg() function \n",
    "#    and rounded to a precision of 2 using .round(2)\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoanReason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. (1pts) LoanReason Values\n",
    "\n",
    "# df.LoanReason is a categorical variable.\n",
    "\n",
    "# Print the frequency counts of each category, including missing values\n",
    "#   using .value_counts() with dropna=False\n",
    "# (You should see a row for NaN 23 indicating 23 missing values)\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. (2pts) Fill Missing Values in LoanReason\n",
    "\n",
    "# Since this is a categorical variable, instead of creating a \"missing\" dummy column\n",
    "#    we'll simply fill the missing values with the string 'MISSING'\n",
    "\n",
    "# Fill the missing values of LoanReason with the string 'MISSING'\n",
    "# Store back into df as LoanReason or use inplace=True\n",
    "____\n",
    "\n",
    "# Print the number of items in each category in LoanReason, including nan's\n",
    "#   using value_counts with dropna=False\n",
    "# (You should see a row for MISSING but no row for NaN)\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. (2pts) Transform LoanReason Using One-Hot Encoding\n",
    "\n",
    "# Transform the LoanReason column into one-hot encoding using pd.get_dummies().\n",
    "# Use prefix='LoanReason' to prefix the column names.\n",
    "# Leave all other arguments as defaults.\n",
    "# Store the resulting dataframe as df_loanreason\n",
    "____\n",
    "\n",
    "# Display the first 3 rows of df_loanreason to confirm the transformation.\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. (2pts) Join df_features With df_loanreason\n",
    "\n",
    "# Join the existing df_features dataframe with df_loanreason\n",
    "# Store the result back into df_features\n",
    "____\n",
    "\n",
    "# Display the transpose of the first 3 rows of df_features\n",
    "# As the dataframe is getting too wide to display in a notebook\n",
    "#    instead display the transpose of the first 3 rows of df_features\n",
    "#    so that rows become columns and columns rows\n",
    "# Recall: to get the transpose of a DataFrame or Series use .T\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. (2pts) Scale and Store Ages\n",
    "\n",
    "# The last variable we'll deal with the numeric variable Age.\n",
    "\n",
    "# Since there are no missing values, we can scale and store Age\n",
    "# Use a new StandardScaler (with default arguments) to fit and transform Age\n",
    "# Store as Age_scaled in df_features\n",
    "____\n",
    "\n",
    "# Print the min and max values for df.Age using .agg()\n",
    "____\n",
    "\n",
    "# Print the min and max value for df_features.Age_scaled using .agg()\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17. (1pts) Create Age Bin Edges for Age\n",
    "\n",
    "# We'll also transform Age into a categorical variable using binning.\n",
    "# Note that this is for practice and there aren't any clear indications\n",
    "#    in the data that we should be binning this way.\n",
    "\n",
    "# We'll bin Age into 3 three equal sized groups\n",
    "# To get the bin edges use np.quantile()\n",
    "# The input array is a=df.Age\n",
    "# The quantiles we want are q=[0,.33,.66,1]\n",
    "# Store the bin edges as age_bins\n",
    "____\n",
    "\n",
    "# Print the bin edges\n",
    "# The min (left-most number) and max (right-most) should match \n",
    "#   the min and max seen printed above\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18. (2pts) Bin Age\n",
    "\n",
    "# Use pd.cut() to bin Age\n",
    "# Use the age_bins list we created above for the bin edges.\n",
    "# Set right=True to include right edge in each bin.\n",
    "# Set include_lowest=True to include the minimum value in the first bin.\n",
    "# Set the bin labels as ['low','medium','high'].\n",
    "# Store as age_binned\n",
    "____\n",
    "\n",
    "# Print the first 3 rows of age_binned\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19. (3pts) Transform Age Bins using One-Hot Encoding and Join to Features\n",
    "\n",
    "# Use pd.get_dummies() to encode age_binned\n",
    "# Use prefix 'Age'.\n",
    "# Store as df_age_binned.\n",
    "____\n",
    "\n",
    "# Join the existing df_features dataframe with df_age_binned.\n",
    "# Store the result back into df_features\n",
    "____\n",
    "\n",
    "# Display the transpose of the first 3 rows of df_features\n",
    "# Should see 11 rows and 3 columns\n",
    "# Note that all features are numeric\n",
    "____\n",
    "\n",
    "# Assert that there are no missing values in df_features\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20. (1pts) Transform Target \n",
    "\n",
    "# The target we're interested in predicting is df.WasTheLoanApproved.\n",
    "# This is a categorical variable taking the values Y for yes and N for no\n",
    "\n",
    "# Transform the target df.WasTheLoanApproved\n",
    "#    into integers 0 for N and 1 for Y using .map()\n",
    "# Recall .map() takes a dictionary of key:value pairs where\n",
    "#   keys   = what you want to map from\n",
    "#   values = what you want to map to\n",
    "# Store the resulting Series in y\n",
    "____\n",
    "\n",
    "# Print the proportion of positives (1's) in y with a precision of 2\n",
    "# Note that there are more 1's than 0's\n",
    "# We can use this as our baseline accuracy\n",
    "# We'd like to find a model that does better than this\n",
    "print(f'proportion of positives: {____}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21. (1pts) Split the Data\n",
    "\n",
    "# Before we continue we should split up our data into a train and test set\n",
    "\n",
    "# import train_test_split from sklearn\n",
    "____\n",
    "\n",
    "# Generate a training and test set from df_features and y\n",
    "#   with test_size=.1, stratify=y, and random_state=123\n",
    "# Store in X_train,X_test,y_train,y_test\n",
    "____\n",
    "\n",
    "# Print the shape of X_train (should be 596 rows, 11 columns).\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#22. (4pts) Rank Feature Importance Using Random Forest Classifier\n",
    "\n",
    "# Import RandomForestClassifier from sklearn\n",
    "____\n",
    "\n",
    "# Instantiate a RandomForestClassifier object\n",
    "# Use n_estimators=10, random_state=123, n_jobs=-1 and all other arguments as their default.\n",
    "# Store as rfc\n",
    "____\n",
    "\n",
    "# Fit rfc on the training set\n",
    "____\n",
    "\n",
    "# The feature_importances_ stored in rfc are in the order of the columns of df_features\n",
    "# Create a new Series with values from rfc.feature_importances_\n",
    "#    with the index=X_train.columns\n",
    "# Store in rfc_feature_importances\n",
    "____\n",
    "\n",
    "# Display feature_importances sorted by the importance descending\n",
    "# Note that the informative features are RequestedAmount_logscaled and Age_scaled\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 23. (3pts) Feature Selection with SelectFromModel\n",
    "\n",
    "# Import SelectFromModel from sklearn\n",
    "____\n",
    "\n",
    "# Instantiate a SelectFromModel transformer with\n",
    "#   rfc as the estimator \n",
    "#   threshold='mean' (the default)\n",
    "#   prefit=True (as we've already trained it above)\n",
    "# Store as sfm\n",
    "____\n",
    "\n",
    "# Show the selected features using X_train.columns and sfm.get_support()\n",
    "# Recall that sfm.get_support() returns a boolean mask over the features\n",
    "#   with a value of True where the feature has been selected\n",
    "# The features shown should be the top 2 features listed in the previous cell\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24. (2pts) Transform Data Using Selected Features\n",
    "\n",
    "# Create a new dataset using only the features selected in the previous step.\n",
    "# Use sfm to transform X_train and store as X_train_fs\n",
    "____\n",
    "\n",
    "# Use sfm to transform X_test and store as X_test_fs\n",
    "____\n",
    "\n",
    "# Print the shape of X_train_fs (should be 596 rows, 2 columns).\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25. (2pts) Train and Evaluate Model On Selected Features\n",
    "\n",
    "# Instantiate a new RandomForestClassifier\n",
    "#   with n_estimators=10, max_depth=3 and n_jobs=-1\n",
    "# Store in rfc_fs\n",
    "____\n",
    "\n",
    "# Train the rfc_fs model on X_train_fs and y_train\n",
    "____\n",
    "\n",
    "# Print the accuracy achieved by rfc_fs on both \n",
    "#   the training (X_train_fs,y_train) and test set (X_test_fs,y_test) \n",
    "#   with precision of 2 decimal places in both cases\n",
    "# The model will perform poorly on both, especially test. We need more data and features!\n",
    "print(f'training accuracy: {____}')\n",
    "print(f'test accuracy    : {____}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 26. (2pts) Reduce Dataset to 3D Using PCA\n",
    "\n",
    "# Import PCA from sklearn\n",
    "____\n",
    "\n",
    "# Instantiate a pca object with\n",
    "#   n_components=3\n",
    "#   random_state=123\n",
    "# Store as pca\n",
    "____\n",
    "\n",
    "# Fit and transform the X_train to 3d using pca\n",
    "# Store in X_train_pca\n",
    "____\n",
    "\n",
    "# Transform (but don't fit!) the X_test to 3d using the trained pca\n",
    "# Store in X_test_pca\n",
    "____\n",
    "\n",
    "# Print the ratio of variance explained by each component \n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 27. (1pts) Plot the First 2 Dimensions of the PCA Transformation\n",
    "\n",
    "# Use sns.scatterplot to plot the PCA transformed training set\n",
    "#   with the first column on the x-axis and the second column on the y-axis\n",
    "#   colored by (hue=) the target y_train\n",
    "____\n",
    "\n",
    "# The white bands you see are due to our one-hot features.\n",
    "# Note that the target categories are still very mixed\n",
    "# Our models will have a difficult time with the data as is.\n",
    "# Additional features and feature engineering would be needed for this task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eods-f21",
   "language": "python",
   "name": "eods-f21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
